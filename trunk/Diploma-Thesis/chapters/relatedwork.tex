\chapter{Verwandte Arbeiten}
\label{chap:relwork}
 \phantom{Liste Bücher die nicht zitiert werden: \cite{RTR3}\cite{gpugems1}\cite{gpugems2}\cite{gpugems3}\cite{cgtutorial}}
\todo[size=\small, color=green!40, inline]{Kapitel: Review von Tim}%
An dieser Stelle werden einige Veröffentlichungen vorgestellt, die sich mit ähnlichen Problemstellungen befassen, wie die vorliegende Arbeit.
\todo[size=\small, color=yellow!40, inline]{Kapitel: Needs Human Proofreaders!}%
\section{Paralleles Rendering}
\label{sec:relwork:parrender}
Beim parallelen Rendering geht es um die Verteilung von Aufgaben in einer Netzwerktopologie. Dies kann entweder durch einfaches Multithreading oder durch einen Rechnerverbund (Cluster) geschehen. Dabei wird meist das Rendern selbst in verschiedene Unteraufgaben geteilt. Eine weitere Möglichkeit ist auch die Trennung von Aufgaben, die im Eigentlichen nichts miteinander zu tun haben, wie zum Beispiel Trennung der Datenverwaltung von der Bilderzeugung.\\
In den letzten Jahren wurden viele Möglichkeiten zur Verteilung der Aufgaben in parallelen Rendersystemen veröffentlicht. Molnar et al. \cite{molnar} klassifizieren Parallelisierungsstrategien in drei Kategorien, in Abhängigkeit davon, an welcher Stelle der Rendering-Pipeline Polygone nach ihrer Sichtbarkeit sortiert werden.

Sort-First-Algorithmen teilen den Bildschirm in disjunkte Regionen ein (Kacheln) und weisen jedem Renderer eine solche Kachel zu. Jeder Renderer ist für die gesamte Bilderzeugung in diesem einem Bildabschnitt zuständig. Geometrische Objekte werden zu den Renderern übertragen, wenn diese in der jeweiligen Kachel sichtbar sind. Sort-First-Renderer nutzen die Frame-to-Frame-Kohärenz gut aus, da nur wenige geometrische Primitive zwischen einzelnen Frames die Bildausschnitte wechseln. Die Rendering-Algorithmen sind bei Sort-First frei wählbar, da jeder Renderer die vollständige Geometrie für seine Kachel besitzen muss. Allerdings kann es sein, dass sich viele geometrische Objekte der gesamten Szene auf wenige Kacheln verteilen, wodurch die Lastverteilung aus dem Gleichgewicht gerät. Ein weiterer Nachteil ist, dass Objekte von mehreren Renderern gezeichnet werden müssen.

Sort-Middle-Algorithmen teilen den Rendervorgang in Geometrie- und Rasteroperationen auf. Geometrische Objekte werden auf verschiedene Geometrieprozessoren verteilt und Bildteile werden an Rasterizer verteilt. Transformation und Beleuchtung geschieht auf den Geometrieprozessoren, welche ihre Ergebnisse dann an die entsprechenden Rasterizer schicken. Dieses Verfahren eignet sich am Besten für High-End Grafik-Workstations, welche aus eng-gekoppelten Systemen bestehen. Für den Versand der Daten zwischen Geometrieprozessoren und den Rasterizern ist jedoch eine hohe Bandbreite vonnöten.

Sort-Last-Algorithmen verteilen geometrische Objekte an einzelne Renderer. Ein Renderer berechnet Pixelwerte für seine Untermenge an Objekten und verteilt die Farb- und Tiefenwerte an Prozesse, die diese dann vereinigen. Bei großen Objektmengen bietet sich dieses Verfahren an, da jedes geometrische Objekt genau einmal gerendert wird. Für den Pixeltransfer wird allerdings eine hohe Bandbreite vom Netzwerk abverlangt. Da die genaue Tiefe eines Pixels erst bei der Vereinigung der Pixel ermittelt werden kann, ist Sort-Last für einige Rendertechniken, wie Transparenz oder Antialiasing, ungeeignet.

Samanta et al. \cite{samanta} haben einen hybriden Renderer entwickelt, welcher Sort-First und Sort-Last-Ansätze kombiniert. Geometrische Objekte werden gruppiert nach der minimalen Überlappung ihrer Boundingboxen und anschließend auf Sort-Last Rechenknoten verteilt. Besitzen mehrere Knoten überlappende Objekte, werden die berechneten Tiefenwerte ausgetauscht und mit ihren eigenen ergänzt. So erhält jeder Renderer ein nahezu korrektes Tiefenabbild. Die fertigen Farbpixel werden am Ende an einen speziellen Kachelrenderer weitergeleitet, welcher lediglich die Kacheln zusammensetzt und darstellt. Der gesamte Pixelversand wird mittels peer-to-peer im Netz\-werk verteilt, wodurch eine verbesserte Netzlastnutzung mit verringerten Latenzen möglich ist.\\
2002 stellten Baxter et al. \cite{baxter} ein paralleles Walkthrough-System "`Gigawalk"' vor. Das System fasst Objekte zu sogenannten Clustern zusammen. In einem minimalen Spannbaum wird die minimale Ausdehnung eines Clusters berechnet um mehrere Cluster miteinander verbinden zu können. Eine hierarchische Gruppierung ergibt sich über die Boundingboxen der einzelnen Cluster, woraus ein Szene-Graph erzeugt wird. Aus dieser Hierarchie werden verschiedene Levels-of-Detail (LOD)\cite{hlod} generiert, um bei komplexen Objekten flexibel bleiben zu können. All dies geschieht im Preprocessing. Durch Frustum-Culling werden zur Laufzeit Potentially Visible Sets \cite{RTR3} zusammengestellt, also Mengen an wahrscheinlich sichtbaren Clustern. Um zu verhindern, dass verdeckte Objekte gezeichnet werden, wird ein hierarchischer Z-Buffer verwendet, welche ein Occlusion-Culling durchführt. Als hierarchische Occluder kommen dabei die berechneten LODs zum Einsatz. Die gesamte Kommunikation erfolgt durch Shared-Memory-Queues. Dieses System wurde mit SGI Onyx Workstations getestet und kommt bei 80 Millionen Dreiecken auf 11-50 Bilder pro Sekunde.\\
Ein anderer Ansatz wird von \cite{dpbp} verfolgt, um mit Multicluster-Systemen zu arbeiten, also Instituts-übergreifend zu rendern. Dabei kommt ein Sort-First-Algorithmus zum Einsatz, mit dem auch gekachelte Bildsysteme angesteuert werden können, wie die CAVE \cite{cave} oder die Powerwall am Heinz-Nixdorf Institut. Im Preprocessing wird die räumliche Verteilung von Objekten zu Clustern zusammengefasst. Um eine hierarchische Gewichtung zu erhalten, werden die projizierten Cluster-Boundingboxen in sogenannten Pixel-Buckets sortiert und gerendert.\\
Ähnlich arbeitet man bei \cite{DBLP:journals/ijvr/YinJSZ06}. Da dort Terraindaten berechnet, werden kommt dieses System ohne Frustum-Culling aus, da solche Daten keine große Tiefenkomplexität besitzen. Bemerkenswert bei dieser Arbeit ist jedoch, dass die Datenstruktur, ein Quadtree, im Preprocessing erzeugt wird und binär auf Festplatte gespeichert wird. Somit wird ein ständiges Erzeugen einer geeigneten räumlichen Aufteilung des Modells hinfällig. Der Fokus der Arbeit liegt allerdings in der Kalibrierung des Beamerystems, welches unter Verwendung von Alpha-Blending sichtbare Kanten zwischen einzelnen Displays entfernt.\\
Um der potenziell ungleich verteilten Last bei Sort-First-Ansätzen entgegen zu wirken, haben Abraham et al. \cite{abraham} einen parallelen Renderer entwickelt der die Kachelgrößen automatisch anhand der letzten Renderzeit der jeweiligen Kachel anpasst. Jedem Renderknoten im Netzwerk liegt dabei das 3D-Modell vollständig vor. Ein Thread ist immer für die Bilderzeugung zuständig, während ein Weiterer sich um den Versand und Empfang von Nachrichten im Netzwerk kümmert. Der Renderknoten, der in einem Frame als Erster seine Bildkachel abliefert, bekommt im nächsten Frame die Kachel mit dem höchsten Aufwand zugeteilt. Dieses Kacheltauschen ist jedoch für Out-of-Core Systeme ungeeignet, da die Objektdaten dann ebenfalls umverteilt werden müssen. Außerdem ist ein homogener Rechencluster notwendig, damit Unterschiede in den Rechenzeiten nicht bedingt durch unterschiedliche Hardware sind. Das Testsystem besteht aus 10 Pentium 4-PCs mit je 512MB RAM, einer GeForce 4 Grafikkarte und einer GigaBit Ethernet Anbindung.

\section{Out-of-Core Rendering}
\label{sec:relwork:oocrender}
Von Out-of-Core Rendering wird gesprochen, wenn ein zu renderndes 3D-Modell nicht vollständig in den Arbeitsspeicher der Grafikkarte passt. Man könnte Teile des Modells entfernen, bis der verbleibende Rest nicht mehr zu groß für den Speicher ist. Teilweise wird dies auch gemacht, führt jedoch zu Bildfehlern, da Teile fehlen können, die eigentlich sichtbar sind. Eine weitere Möglichkeit besteht darin, kleinere Mengen des Modells einzeln zu rendern und am Ende die Bilder zu vereinigen. Das resultierende Bild ist so zwar fehlerfrei, allerdings geht dieses Verfahren zulasten der Rendergeschwindigkeit. Interaktive Bildraten können so wahrscheinlich nicht erzielt werden.

Manocha et al. \cite{manocha} stellen ein Out-of-Core System vor, mit dem sich interaktive Bildraten in einem Verbund mehrerer SGI Workstations erzielen lassen. Ein Prefetching-Thread versucht Objekte im Voraus zu laden, wodurch Popping-Artefakte reduziert werden. Durch Erweiterung des eigentlichen Frustums verhält sich die Kamera ähnlich wie bei einem Weitwinkelobjektiv. Durch diese Technik werden Objekte an den Rändern schon geladen, obwohl sie noch nicht sichtbar sind. Somit sind sie im nächsten Frame verfügbar, wo sie vielleicht benötigt werden. Zusätzlich werden beim Prefetching Objekte die nur im erweiterten Frustum liegen durch den Winkel zum Zentrum der Kamera priorisiert, wodurch Objekte, welche näher am Kamerazentrum liegen, bevorzugt werden. Wie viel größer das erweiterte Frustum ist, hängt von der aktuellen Bewegungsgeschwindigkeit ab.\\
Auch Wagner et al. \cite{wagner1}, \cite{wagner2} speichern eine hierarchische Repräsentation des Modells in einem Preprocessing-Schritt auf der Festplatte ab. Ihr System ist eine parallele Erweiterung ihres eigenen iWalk-Renderers \cite{iwalk}. Mit Dieser sind sie in der Lage, Bilder mit sehr hohen Auflösungen (4096$\times$3072) zu erstellen. Das System arbeitet in einem Rechencluster von 16 Pentium4-PCs mit je 512MB RAM. Jeder Rechenknoten besitzt ein festgelegtes Kontingent an Dreiecken und Speicherverbrauch. Unter Einhaltung dieses Kontingents wird für jeden Frame die sichtbare Geometrie in Form von Octree-Knoten über Prioritized-Layered Projections (PLP) \cite{plp} bestimmt. Ein Vorteil von PLP besteht darin, dass eine hierarische Struktur des Modells während des Preprocessings erzeugt werden kann. Dadurch können die sichtbaren Octree-Knoten zur Laufzeit bestimmt werden, ohne dabei auf die tatsächliche Szenengeometrie zugreifen zu müssen. Es gibt einen separaten Caching-Thread, der Objekte eine Weile im Speicher belässt und sich um Prefetching kümmert. Objektanfragen den aktuellen Frame betreffend werden dabei jedoch bevorzugt. Als Verdrängungsstrategie kommt Least-Recently-Used (LRU) zum Einsatz.\\
Bei Wald et al. \cite{wald} wird auch das Modell der Boeing 777 gerendert (ca. 350.000.000 Dreiecke), allerdings mit einem Raytracing-Verfahren \cite{RTR3}. Hierbei wird die gesamte Boeing auf einem Dual-Core 1.8GHz Opteron-PC mit 6GB RAM gerendert. Die Speicherverwaltung arbeitet mittels Memory-Mapped-I/O, womit Teile der Festplatte direkt in den Arbeitsspeicher gespiegelt werden können. Da ständiges Ein- und Auslagern der Speicherseiten sehr teuer für einen Raytracer ist, hat man sich entschieden, selbst eine solche Verwaltung zu entwickeln. Im Vorfeld werden sogenannte Geometrie-Proxies erstellt, die wegen ihrer geringen Größe vollständig im Speicher des Renderers abgelegt werden. Wird ein Modellteil benötigt, ist die Speicheradresse bekannt, an der es sich befinden müsste. Ist es vorhanden, wird es gerendert, ansonsten wird aus einer STL-Map über die Adresse der entsprechende Proxy gerendert. Mit entsprechenden Systemaufrufen können Speicherseiten direkt ausgelagert werden oder länger als üblich aktiv im Speicher gehalten werden. Durch dieses System wurden bei einer Auflösung von 640$\times$480 3-7 Bilder pro Sekunde erreicht.

% EOF
%
